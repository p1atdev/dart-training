{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/huggingface/cache\"\n",
    "\n",
    "TOKENIZER_NAME = \"./dart-tokenizer-20240219\"\n",
    "\n",
    "DATASET_NAME = \"isek-ai/danbooru-tags-2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/p1atdev/dart-tokenizer-v1:\n",
      "- tokenization_dart.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d64754bde644adb616f77e4cc86152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"p1atdev/dart-tokenizer-v1\", trust_remote_code=True, cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load filtered tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"danbooru-tags-filtered-20240219\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'copyright', 'character', 'artist', 'general', 'meta', 'rating', 'score', 'created_at'],\n",
       "    num_rows: 5293004\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as parse_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2005-10-16T00:19:32.000+09:00',\n",
       " '2005-10-28T12:38:48.000+09:00',\n",
       " '2005-10-03T09:55:56.000+09:00',\n",
       " '2005-10-26T07:26:19.000+09:00',\n",
       " '2005-10-04T05:05:17.000+09:00',\n",
       " '2005-10-03T12:19:24.000+09:00',\n",
       " '2005-10-16T07:18:57.000+09:00',\n",
       " '2005-10-23T18:52:41.000+09:00',\n",
       " '2005-10-18T10:17:52.000+09:00',\n",
       " '2005-10-01T05:25:38.000+09:00']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"created_at\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_date(\"2005-10-16T00:19:32.000+09:00\").year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'copyright', 'character', 'artist', 'general', 'meta', 'rating', 'score', 'created_at'],\n",
       "    num_rows: 2531560\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use after 2020\n",
    "ds = ds.filter(lambda x: parse_date(x[\"created_at\"]).year >= 2020, batched=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_map = {\n",
    "    \"g\": \"rating:general\",\n",
    "    \"s\": \"rating:sensitive\",\n",
    "    \"q\": \"rating:questionable\",\n",
    "    \"e\": \"rating:questionable\",\n",
    "}\n",
    "\n",
    "rating_parent_tag_map = {\n",
    "    \"g\": \"rating:sfw\",\n",
    "    \"s\": \"rating:sfw\",\n",
    "    \"q\": \"rating:nsfw\",\n",
    "    \"e\": \"rating:nsfw\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    [\n",
    "        tokenizer.convert_tokens_to_ids(token) != tokenizer.unk_token\n",
    "        for token in list(rating_map.values()) + list(rating_parent_tag_map.values())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"<|bos|>\"\n",
    "EOS = \"<|eos|>\"\n",
    "\n",
    "RATING_BOS = \"<rating>\"\n",
    "RATING_EOS = \"</rating>\"\n",
    "\n",
    "COPYRIGHT_BOS = \"<copyright>\"\n",
    "COPYRIGHT_EOS = \"</copyright>\"\n",
    "\n",
    "CHARACTER_BOS = \"<character>\"\n",
    "CHARACTER_EOS = \"</character>\"\n",
    "\n",
    "GENERAL_BOS = \"<general>\"\n",
    "GENERAL_EOS = \"</general>\"\n",
    "\n",
    "INPUT_END = \"<|input_end|>\"  # boundary of input and output\n",
    "\n",
    "VERY_VAGUE = \"<|very_vague|>\"\n",
    "VAGUE = \"<|vague|>\"\n",
    "DETAILED = \"<|detailed|>\"\n",
    "VERY_DETAILED = \"<|very_detailed|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    [\n",
    "        tokenizer.convert_tokens_to_ids(token) != tokenizer.unk_token\n",
    "        for token in [\n",
    "            BOS,\n",
    "            EOS,\n",
    "            RATING_BOS,\n",
    "            RATING_EOS,\n",
    "            COPYRIGHT_BOS,\n",
    "            COPYRIGHT_EOS,\n",
    "            CHARACTER_BOS,\n",
    "            CHARACTER_EOS,\n",
    "            GENERAL_BOS,\n",
    "            GENERAL_EOS,\n",
    "            INPUT_END,\n",
    "            VERY_VAGUE,\n",
    "            VAGUE,\n",
    "            DETAILED,\n",
    "            VERY_DETAILED,\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'copyright', 'character', 'artist', 'general', 'meta', 'rating', 'score', 'created_at'],\n",
       "    num_rows: 2531560\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tag_manager import TagManger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_manaer = TagManger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1girl',\n",
       " '2girls',\n",
       " '3girls',\n",
       " '4girls',\n",
       " '5girls',\n",
       " '6+girls',\n",
       " '1boy',\n",
       " '2boys',\n",
       " '3boys',\n",
       " '4boys',\n",
       " '5boys',\n",
       " '6+boys',\n",
       " '1other',\n",
       " '2others',\n",
       " '3others',\n",
       " '4others',\n",
       " '5others',\n",
       " '6+others',\n",
       " 'no humans']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_tags = tag_manaer.tags[\"PEOPLE\"]\n",
    "people_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_tags(tag_text: str):\n",
    "    tags = tag_text.split(\", \")\n",
    "    np.random.shuffle(tags)\n",
    "    return \", \".join(tags)\n",
    "\n",
    "\n",
    "generator = np.random.default_rng()\n",
    "\n",
    "\n",
    "def split_general_tags(\n",
    "    general_tags: list[str],\n",
    "    range_min: float = 0,\n",
    "    range_max: float = 0.75,\n",
    "    people_dropout_rate: float = 0.05,\n",
    "):\n",
    "    # isolate people tags\n",
    "    isloted_tags, other_tags = [], []\n",
    "\n",
    "    # 5% の確率で人物系のタグ全部ランダムにしてしまう\n",
    "    if generator.random() <= people_dropout_rate:\n",
    "        other_tags = general_tags\n",
    "    else:\n",
    "        for tag in general_tags:\n",
    "            if tag in people_tags:\n",
    "                isloted_tags.append(tag)\n",
    "            else:\n",
    "                other_tags.append(tag)\n",
    "\n",
    "    # 範囲からランダムな割合を選択\n",
    "    ratio = np.random.uniform(range_min, range_max)\n",
    "\n",
    "    # 配列の要素をランダムに並べ替え\n",
    "    np.random.shuffle(other_tags)\n",
    "\n",
    "    # 選ばれた割合に基づいて分割位置を決定\n",
    "    split_index = int(len(other_tags) * ratio)\n",
    "\n",
    "    # 配列を2つのグループに分ける\n",
    "    input_group = isloted_tags + other_tags[:split_index]\n",
    "    output_group = other_tags[split_index:]\n",
    "\n",
    "    # shuffle only input\n",
    "    np.random.shuffle(input_group)\n",
    "\n",
    "    # sort output group\n",
    "    output_group = sorted(output_group)\n",
    "\n",
    "    return \", \".join(input_group), \", \".join(output_group)\n",
    "\n",
    "\n",
    "# 詳細度のタグを取得する\n",
    "def get_detail_level_tag(count: int):\n",
    "    if count <= 10:\n",
    "        return VERY_VAGUE\n",
    "    elif count <= 20:\n",
    "        return VAGUE\n",
    "    elif count <= 40:\n",
    "        return DETAILED\n",
    "    else:\n",
    "        return VERY_DETAILED\n",
    "\n",
    "\n",
    "def concat_tags(examples):\n",
    "    all_tags = []\n",
    "\n",
    "    for i, _ in enumerate(examples[\"id\"]):\n",
    "        rating = examples[\"rating\"][i]\n",
    "        copyright = examples[\"copyright\"][i]\n",
    "        character = examples[\"character\"][i]\n",
    "        general = examples[\"general\"][i]\n",
    "\n",
    "        assert rating is not None\n",
    "        assert general is not None\n",
    "\n",
    "        rating, rating_parent = rating_map[rating], rating_parent_tag_map[rating]\n",
    "        rating = \", \".join([rating, rating_parent])\n",
    "\n",
    "        if copyright is None:\n",
    "            copyright = \"\"\n",
    "        if character is None:\n",
    "            character = \"\"\n",
    "\n",
    "        rating = shuffle_tags(rating)\n",
    "        copyright = shuffle_tags(copyright)\n",
    "        character = shuffle_tags(character)\n",
    "\n",
    "        general_tags = general.split(\", \")\n",
    "\n",
    "        detail_level = get_detail_level_tag(len(general_tags))\n",
    "\n",
    "        input_tags, output_tags = split_general_tags(\n",
    "            general_tags, range_min=0, range_max=0.75\n",
    "        )\n",
    "\n",
    "        tag_text = \"\".join(\n",
    "            [\n",
    "                BOS,\n",
    "                RATING_BOS,\n",
    "                rating,\n",
    "                RATING_EOS,\n",
    "                COPYRIGHT_BOS,\n",
    "                copyright,\n",
    "                COPYRIGHT_EOS,\n",
    "                CHARACTER_BOS,\n",
    "                character,\n",
    "                CHARACTER_EOS,\n",
    "                GENERAL_BOS,\n",
    "                detail_level,\n",
    "                input_tags,\n",
    "                INPUT_END,\n",
    "                output_tags,\n",
    "                GENERAL_EOS,\n",
    "                EOS,\n",
    "            ]\n",
    "        )\n",
    "        all_tags.append(tag_text)\n",
    "\n",
    "    return {\"tag_text\": all_tags}\n",
    "\n",
    "\n",
    "ds = ds.map(concat_tags, batched=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|bos|><rating>rating:nsfw, rating:questionable</rating><copyright>idolmaster, idolmaster shiny colors</copyright><character>mayuzumi fuyuko, producer (idolmaster)</character><general><|detailed|>1boy, 1girl, brown eyes<|input_end|>3koma, barefoot, black hair, black ribbon, blunt bangs, blush, breasts, comic, faceless, faceless male, heart, hetero, leg lock, long hair, looking at viewer, mating press, medium breasts, missionary, nude, open mouth, ribbon, sex, short hair, spoken heart, toe scrunch, tsundere, two side up</general><|eos|>',\n",
       " '<|bos|><rating>rating:questionable, rating:nsfw</rating><copyright>neon genesis evangelion, end of evangelion</copyright><character>souryuu asuka langley, mass production eva</character><general><|detailed|>mecha on girl, breasts, long fingers, 1girl, interspecies, highleg<|input_end|>adapted costume, alternate size, blue eyes, clothes lift, dress, drooling, fingering, hetero, highleg dress, interface headset, lips, long hair, monster, muscular, no eyes, no panties, orange hair, pelvic curtain, pelvic curtain lift, plugsuit, pussy, pussy juice, red dress, red legwear, skin tight, skindentation, small breasts, standing, sweat, thick thighs, thighhighs, thighs</general><|eos|>',\n",
       " '<|bos|><rating>rating:questionable, rating:nsfw</rating><copyright>fate (series), fate/grand order</copyright><character>ishtar (fate)</character><general><|very_detailed|>blush, male pubic hair, jewelry, asymmetrical legwear, caressing testicles, penis, toes, black legwear, pubic hair, nude, gangbang, sweat, solo focus, cum, 1girl, multiple boys, 3boys, testicles, barefoot, cum on hair, back, feet<|input_end|>ass, bare shoulders, black hair, closed eyes, cum on body, cum on feet, facial, group sex, hetero, long hair, multiple penises, open mouth, penis on face, pussy, sex, soles, uncensored, uneven legwear, vaginal</general><|eos|>',\n",
       " '<|bos|><rating>rating:sfw, rating:sensitive</rating><copyright>fate/extra ccc, fate (series), fate/extra</copyright><character>bb (fate/extra), bb (fate)</character><general><|detailed|>ponytail, finger to mouth, looking at viewer, 1girl, ass, breasts, very long hair, competition swimsuit, bow, parted lips, thighhighs, swimsuit, solo, black one-piece swimsuit<|input_end|>back, bare shoulders, black thighhighs, blush, grey background, hair bow, high ponytail, index finger raised, large breasts, long hair, looking back, one eye closed, one-piece swimsuit, open mouth, petals, purple eyes, purple hair, purple one-piece swimsuit, red bow, simple background, sitting, smile, thighs, two-tone swimsuit, v-shaped eyebrows</general><|eos|>',\n",
       " '<|bos|><rating>rating:questionable, rating:nsfw</rating><copyright>league of legends</copyright><character>jinx (league of legends)</character><general><|very_detailed|>very long hair, red eyes, pussy, looking at viewer, purple nails, 1girl, stomach, small breasts, bullet, solo, tattoo, smile, purple gloves, long hair, uneven gloves, purple thighhighs, single thighhigh, aqua hair, armpits, spread legs, nose, uncensored, braid, toned, breasts, thighhighs, nipples, arm behind head<|input_end|>asymmetrical gloves, belt, black gloves, bottomless, elbow gloves, fingerless gloves, gloves, navel, pale skin, squatting, thigh strap, twin braids, v-shaped eyebrows</general><|eos|>',\n",
       " \"<|bos|><rating>rating:questionable, rating:nsfw</rating><copyright>pokemon (classic anime), pokemon, pokemon (anime)</copyright><character>hypno, misty (pokemon)</character><general><|very_detailed|>furry male, cum in pussy, mind control, green eyes, grabbing, heart-shaped pupils, solo focus, furry with non-furry, penis, large breasts, censored, leg up, hetero, pokemon (creature), open mouth, grabbing another's breast, nipples, pokephilia, cum overflow, sweat, upper teeth only, cum, bottomless, 1boy, side ponytail, teeth, grabbing from behind, vaginal, 1girl<|input_end|>bar censor, blush, breasts, furry, heart, hypnosis, interspecies, navel, orange hair, saliva, sex, short hair, symbol-shaped pupils</general><|eos|>\",\n",
       " \"<|bos|><rating>rating:questionable, rating:nsfw</rating><copyright>fate/grand order, fate (series)</copyright><character>minamoto no raikou (fate)</character><general><|detailed|>1girl, parted lips<|input_end|>blush, breasts, collarbone, hair between eyes, hand on another's cheek, hand on another's face, hanging breasts, large breasts, leaning forward, long hair, looking at viewer, nipples, nude, one eye closed, parted bangs, pov, purple eyes, purple hair, simple background, solo focus, thighs, very long hair, white background</general><|eos|>\",\n",
       " '<|bos|><rating>rating:questionable, rating:nsfw</rating><copyright>azur lane</copyright><character>taihou (enraptured companion) (azur lane), taihou (azur lane)</character><general><|very_detailed|>crossed bangs, arms behind head, 1girl, swimsuit, race queen, one side up, sky, checkered flag, eyewear on head, multi-strapped bikini bottom<|input_end|>ahoge, armpits, arms up, bikini, black bikini, black hair, black thighhighs, blue sky, blush, breasts, cleavage, day, flag, huge breasts, jacket, long hair, long sleeves, looking at viewer, micro bikini, multi-strapped bikini top, navel, official alternate costume, one eye closed, open clothes, open jacket, parted lips, red eyes, red jacket, skindentation, smile, solo, sunglasses, thigh strap, thighhighs, underboob</general><|eos|>',\n",
       " '<|bos|><rating>rating:nsfw, rating:questionable</rating><copyright>rwby</copyright><character>ruby rose</character><general><|very_detailed|>ass, short sleeves, short hair, looking at viewer, clothes writing, cleavage, covered nipples, breasts, high heels, toned, cropped torso, upskirt, miniskirt, panties, grey eyes, thighhighs, 1girl, split, abs, blush, standing, clothing cutout, pantyshot, multiple views, necktie, cleavage cutout, white panties, purple footwear, police, blue skirt, police uniform, watson cross, medium breasts<|input_end|>back, black gloves, cropped shirt, crossed legs, full body, garter straps, gloves, navel, parted lips, patreon logo, purple legwear, red hair, sitting, skirt, smile, spread legs, thigh gap, two-tone background, underwear, uniform</general><|eos|>',\n",
       " '<|bos|><rating>rating:nsfw, rating:questionable</rating><copyright>princess connect!</copyright><character>pecorine (princess connect!), karyl (princess connect!)</character><general><|very_detailed|>nude, medium breasts, tongue, tongue out, group sex, streaked hair, multicolored hair, closed eyes, underwear, threesome, animal ears, cat girl, long hair, ffm threesome, standing, side braid, braid, masturbation, completely nude, large breasts, tiara, panty pull, barefoot, multiple girls, strap slip, cat tail, blue eyes, bra pull, on back, breasts, black hair, fingering, cat ears, sex, white hair, 2girls, 1boy, panties, hanging breasts, nipples, bra<|input_end|>arms up, bent over, clothes pull, covering mouth, female masturbation, glint, hetero, low twintails, lying, open mouth, orange hair, sweat, tail, twintails</general><|eos|>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"tag_text\"][100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf1b93bc8f54937807cbaa052ce5959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/808012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk(\"./danbooru-sfttext-20240221\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1fbe3f1582465aa9c50517bfe78523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7988684aa384134a7d0f9386a269862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/422 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8726eec262d946fd90129e1c2d84c352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/422 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc23cd619f342409cac93b7a2f5fd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/422 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36caa4390d004e1ea0968183806046a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/422 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89b95e26fdd447a901421c689ad9be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/422 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79cbd3a89704d8b99f5f0211c5b6443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/422 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/p1atdev/dart-sft-20240221/commit/69b7ff78c91a26b07653df96455d59aa13f8827c', commit_message='Upload dataset', commit_description='', oid='69b7ff78c91a26b07653df96455d59aa13f8827c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"p1atdev/dart-sft-20240221\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'copyright', 'character', 'artist', 'general', 'meta', 'rating', 'score', 'created_at', 'tag_text'],\n",
       "    num_rows: 6524302\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_from_disk(\"./danbooru-tagtext-20240219\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 4, 49, 46, 5, 6, 3252, 931, 7, 8, 12129, 3, 14631, 9, 10, 60268, 61285, 38967, 39211, 47069, 50914, 58886, 55184, 42984, 32162, 55932, 45343, 25530, 28649, 59997, 24239, 66120, 51088, 46454, 18581, 19188, 65308, 65393, 27019, 67294, 53925, 31442, 21535, 67510, 29954, 32664, 49077, 40333, 45530, 43500, 40044, 32039, 32815, 67386, 36057, 11, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    \"<|bos|><rating>rating:nsfw, rating:questionable</rating><copyright>dragon quest, dragon quest v</copyright><character>hero's daughter (dq5), king slime, slime (dragon quest)</character><general>1girl, :d, ^_^, ass, bestiality, black eyes, blob, blonde hair, blue skin, blush, boots, bow, closed eyes, clothes lift, colored skin, crown, dress, dress lift, female orgasm, full body, gloves, hair bow, hat, head back, loli, no panties, open mouth, orgasm, rape, see-through, see-through body, short dress, simple background, sketch, smile, solo, tears, top-down bottom-up, vaginal, wince</general><|eos|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'copyright',\n",
       " 'character',\n",
       " 'artist',\n",
       " 'general',\n",
       " 'meta',\n",
       " 'rating',\n",
       " 'score',\n",
       " 'created_at']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ds.column_names\n",
    "column_names.remove(\"tag_text\")\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cdbb7ee8714128a61f336d6609123b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5293004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tag_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 5293004\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(examples):\n",
    "    tokenized = tokenizer(examples[\"tag_text\"])\n",
    "    input_ids = tokenized.input_ids\n",
    "\n",
    "    # check not to have unknown copyright or character tags\n",
    "    for i, token_ids in enumerate(input_ids):\n",
    "        unk_token_idx = [\n",
    "            i for i, x in enumerate(token_ids) if x == tokenizer.unk_token_id\n",
    "        ]\n",
    "        general_bos_idx = token_ids.index(10)\n",
    "\n",
    "        if any([unk < general_bos_idx for unk in unk_token_idx]):\n",
    "            raise Exception(\"unk before general!\")\n",
    "\n",
    "    input_ids = [  # remove unk token\n",
    "        [token_id for token_id in item if token_id != tokenizer.unk_token_id]\n",
    "        for item in input_ids\n",
    "    ]\n",
    "\n",
    "    return {**tokenized}\n",
    "\n",
    "\n",
    "ds = ds.map(tokenize_text, batched=True, remove_columns=column_names)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tag_text': '<|bos|><rating>rating:sfw, rating:sensitive</rating><copyright>to heart (series), to heart 2</copyright><character>himeyuri ruri, himeyuri sango, ilfa (to heart)</character><general>3girls, animal ears, bun cover, buruma, cat ears, cat tail, comic, double bun, greyscale, gym uniform, hair bun, monochrome, mouse ears, mouse tail, multiple girls, tail, thighhighs</general><|eos|>',\n",
       " 'input_ids': [0,\n",
       "  4,\n",
       "  48,\n",
       "  45,\n",
       "  5,\n",
       "  6,\n",
       "  499,\n",
       "  2647,\n",
       "  7,\n",
       "  8,\n",
       "  15158,\n",
       "  7845,\n",
       "  4179,\n",
       "  9,\n",
       "  10,\n",
       "  47498,\n",
       "  24847,\n",
       "  23653,\n",
       "  63183,\n",
       "  34279,\n",
       "  29591,\n",
       "  46121,\n",
       "  36792,\n",
       "  43736,\n",
       "  31133,\n",
       "  33982,\n",
       "  56065,\n",
       "  17149,\n",
       "  30195,\n",
       "  18572,\n",
       "  45564,\n",
       "  19477,\n",
       "  11,\n",
       "  1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1894256f5c334b7ba97c6f06af05c0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/9 shards):   0%|          | 0/5293004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk(\"./danbooru-tokenized-20240219\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### push to hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"./danbooru-tokenized-20240219\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e270b38ce0495ba6133f98bd917c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5d609e812443d1a8591a426b7a3af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7816e9ff78a748e1a6689b577266b6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f7b653df7c4454b7ce0df33e09d9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb80a84f51a43e992be5ae0ed1673a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969f9b33da4943499099eeb83916ac4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8563789b6824edea1f199a5ace9b04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a1ec7ed8f41258b6a81752f7e7168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4c199a28404baca380210f4d6b013e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343fb61a735c441cb4086c1df683f13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/589 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/p1atdev/dart-tokenized-pretrain-20240219/commit/013e3afc021ce12f418ba510be15cc56716caa85', commit_message='Upload dataset', commit_description='', oid='013e3afc021ce12f418ba510be15cc56716caa85', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"p1atdev/dart-tokenized-pretrain-20240219\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
